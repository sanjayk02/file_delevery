import json
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
import matplotlib.pyplot as plt

# Load and preprocess the keyframe data
def load_keyframe_data(file_path):
    with open(file_path, 'r') as f:
        keyframe_data = json.load(f)
    
    joints = list(keyframe_data.keys())
    translations = []
    rotations = []
    
    for joint in joints:
        translations.append(keyframe_data[joint]['translate'])
        rotations.append(keyframe_data[joint]['rotate'])
    
    translations = np.array(translations)
    rotations = np.array(rotations)
    
    return translations, rotations

def normalize_data(data):
    mean = np.mean(data, axis=0)
    std = np.std(data, axis=0)
    normalized_data = (data - mean) / std
    return normalized_data

def create_sequences(data, timesteps):
    sequences = []
    for i in range(len(data) - timesteps):
        sequences.append(data[i:i + timesteps])
    return np.array(sequences)

# Load data
translations, rotations = load_keyframe_data('C:/path/to/save/keyframe_data.json')

# Normalize data
normalized_translations = normalize_data(translations)
normalized_rotations = normalize_data(rotations)

# Create sequences
timesteps = 10
X_translations = create_sequences(normalized_translations, timesteps)
X_rotations = create_sequences(normalized_rotations, timesteps)

# Prepare input and output pairs
y_translations = normalized_translations[timesteps:]
y_rotations = normalized_rotations[timesteps:]

X = np.concatenate((X_translations, X_rotations), axis=-1)
y = np.concatenate((y_translations, y_rotations), axis=-1)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(timesteps, X.shape[2])))
model.add(Dense(y.shape[1]))

model.compile(optimizer='adam', loss='mse')

# Train the model
history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.2)

# Evaluate the model
loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {loss}')

# Make predictions
predictions = model.predict(X_test)

# Visualize training history
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='validation')
plt.legend()
plt.show()
